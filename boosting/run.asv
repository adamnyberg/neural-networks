
% Load face and non-face data and plot a few examples
load faces, load nonfaces
faces = double(faces);
nonfaces = double(nonfaces);
%rng(12345)

nbrHaarFeatures = 100;
nbrTrainExamples = 100;
nbrTestExamples = 3000;
nbrClassifiers = 100;
epsLim = 0.0001;
defaultAlpha = 5;

tic

figure(1)
colormap gray
for k=1:25
    subplot(5,5,k), imagesc(faces(:,:,10*k)), axis image, axis off
end
figure(2)
colormap gray
for k=1:25
    subplot(5,5,k), imagesc(nonfaces(:,:,10*k)), axis image, axis off
end
% Generate Haar feature masks

haarFeatureMasks = GenerateHaarFeatureMasks(nbrHaarFeatures);
figure(3)
colormap gray
for k = 1:25
    subplot(5,5,k),imagesc(haarFeatureMasks(:,:,k),[-1 2])
    axis image,axis off
end



% Create a training data set with a number of training data examples
% from each class. Non-faces = class label y=-1, faces = class label y=1

trainImages = cat(3,faces(:,:,1:nbrTrainExamples),nonfaces(:,:,1:nbrTrainExamples));
xTrain = ExtractHaarFeatures(trainImages,haarFeatureMasks);
yTrain = [ones(1,nbrTrainExamples), -ones(1,nbrTrainExamples)];

testImages = cat(3,faces(:,:,nbrTrainExamples:nbrTrainExamples+nbrTestExamples),nonfaces(:,:,nbrTrainExamples:nbrTrainExamples+nbrTestExamples));
xTest = ExtractHaarFeatures(testImages,haarFeatureMasks);
yTest = [ones(1,nbrTestExamples), -ones(1,nbrTestExamples)];

N = nbrTrainExamples*2;
Ntest = nbrTestExamples*2;
M = nbrHaarFeatures;
T = nbrClassifiers;

d = 1/N*ones(1, N);

% For every classifier..
for t = 1:T
    eps = zeros(M, N);
    pol = ones(M, N);
    
    for m = 1:M % ... test every feature ...
        for n = 1:N % ... with every possible threshold
            
            
            currentThreshold =  xTrain(m,n);
            xT = xTrain(m,:) < currentThreshold; %plocka ut alla thresholds under taur
            mask = find(xT == 0); %hitta alla indexes för 0:or
            xTd = zeros(size(xT));
            xTd = double(xT);
            xTd(mask) = xTd(mask) -1; %ändra alla 0:or till -1
            I = xTd ~= yTrain; %1 vid felklassificeringar, 0 annars
            eps(m,n) = sum(d.*I);
            
            % Extract current threshold
            %currentThreshold = xTrain(m, n);
            
            % Check how well the current theshold performs
            %xThreshold = +(xTrain(m,:) < currentThreshold);
            %xThreshold(xThreshold == 0) = -1;
            %I = +(xThreshold ~= yTrain);
            
            % Calculate the error
            %eps(m, n) = sum(d.*I);
            
            % If the error is too large, flip the polarity
            if eps(m, n) > 0.5
                eps(m, n) = 1 - eps(m, n);
                pol(m, n) = -1;
            end
        end
    end
    
    %Finds the best feature and threshold
    [feature threshold] = find(eps == min(eps(:))); 
    feature = feature(1);
    threshold = threshold(1);
    epsilon = eps(feature, threshold);
    polarity = pol(feature, threshold);
    
    if epsilon > epsLim
        alpha = 0.5*log((1-epsilon)/epsilon);
    else
        alpha = defaultAlpha;
    end
    
    classifiers(:,t) = [epsilon; feature; threshold; polarity; alpha]; % save best classifier
    
    xThreshold = +(xTrain(feature,:) < threshold);
    xThreshold(xThreshold == 0) = -1;
    I = +(xThreshold ~= yTrain);
    
    I(I == 0) = -1;
    if pol(m, n) < 0
        misClass = ~I;
    else
        misClass = I;
    end
    
    [bestFeature col] = find(epsilons == min(epsilons(:))); %Finds all the best 
    bestFeature = bestFeature(1); col = col(1); % Chooses the first 
    bestThreshold = xTrain(bestFeature, col);
    eps_min = epsilons(bestFeature,col);
    bestPolarity = polarities(bestFeature,col);

    %CALCULATE ALPHA FOR WEIGHT UPDATE
    if(eps > eps_min_threshold),
        alpha = log((1-eps_min)/eps_min)/2;
    else
        alpha = alpha_max;
    end
    
    
    error = ((xTrain(feature,:) < threshold) & (yTrain == 1)) | ((xTrain(feature,:) >= threshold) & (yTrain == -1));
    if(polarity == -1)
        error = ~error;
    end
    error = 2*error; %error now consists of 2s and 0s    c
    correct = 1-error;
    
    
    d = d.*exp(-alpha * correct);
    %d = max(0.1/N, d); % Set max limit

    %d = min(10/N, d); % Set min limit
    d = d/sum(d); % Normalize
end

correct = 0;
h1 = 0;

for i = 1:Ntest
    
    h1 = strong(classifiers, xTest(:,i), T);
    
    if h1 == yTest(1,i)
        correct = correct +1;   
    end
    
end

acc = correct/Ntest;


trainingTime = toc;
display(['Time spent training: ' num2str(trainingTime) ' sec'])
display(['Accuracy: ' num2str(acc)])
            
    
        



